{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NUKdEUQHkMxw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Importing the pyplot module from the matplotlib library as plt for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importing the re module for regular expression operations\n",
        "import re\n",
        "\n",
        "# Importing train_test_split function from the sklearn.model_selection module \n",
        "# for splitting data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Importing LabelEncoder class from the sklearn.preprocessing module \n",
        "# for encoding categorical features into numerical values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Importing Tokenizer class from the keras.preprocessing.text module \n",
        "# for tokenizing text data\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Importing pad_sequences function from the keras.preprocessing.sequence module \n",
        "# for padding sequences to a fixed length\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Importing Sequential class from the keras.models module for building sequential models\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Importing Dense, Embedding, LSTM, and SpatialDropout1D layers \n",
        "# from the keras.layers module for constructing neural network layers\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "\n",
        "# Importing to_categorical function from the keras.utils module \n",
        "# for one-hot encoding target variables\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRZzEHbqkaW1",
        "outputId": "aa80cf64-2ce3-419f-f333-d93eb82fac25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-d34a00db5f2b>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['text'] = data['text'].apply(lambda x: x.lower())\n",
            "<ipython-input-5-d34a00db5f2b>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n"
          ]
        }
      ],
      "source": [
        "# Importing the pandas library as pd for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Reading the CSV file 'Sentiment.csv' into a pandas DataFrame named dataset\n",
        "dataset = pd.read_csv('Sentiment.csv')\n",
        "\n",
        "# Creating a boolean mask to select only the columns 'text' and 'sentiment'\n",
        "mask = dataset.columns.isin(['text', 'sentiment'])\n",
        "\n",
        "# Selecting the columns 'text' and 'sentiment' from the dataset using the mask\n",
        "data = dataset.loc[:, mask]\n",
        "\n",
        "# Converting all text data in the 'text' column to lowercase\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "\n",
        "# Removing special characters, punctuation, and symbols from the 'text' column using regular expressions\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "14j__DA_kbSJ"
      },
      "outputs": [],
      "source": [
        "for idx, row in data.iterrows():\n",
        "    \n",
        "    # Replacing 'rt' with empty string (' ') in the first column (index 0) of the current row\n",
        "    row[0] = row[0].replace('rt', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qEYRvHoQke1C"
      },
      "outputs": [],
      "source": [
        "# Setting the maximum number of features to 2000 for tokenization\n",
        "max_fatures = 2000\n",
        "\n",
        "# Initializing a Tokenizer object with the specified maximum number of words (max_fatures) to tokenize sentences\n",
        "# The split parameter is set to ' ' to tokenize words based on space\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "\n",
        "# Fitting the Tokenizer on the text data in the 'text' column of the DataFrame 'data'\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "\n",
        "# Converting the text data into sequences of integers using the fitted Tokenizer\n",
        "# The result is assigned to variable X, which represents the feature matrix\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MRvZo-jCkh0C"
      },
      "outputs": [],
      "source": [
        "# Padding the sequences in the feature matrix X to ensure uniform length\n",
        "X = pad_sequences(X)\n",
        "\n",
        "# Defining the dimension of the embedding layer\n",
        "embed_dim = 128\n",
        "\n",
        "# Defining the number of neurons in the Long Short-Term Memory (LSTM) layer\n",
        "lstm_out = 196"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ox2lpB45klLE"
      },
      "outputs": [],
      "source": [
        "# Function to create a sequential neural network model\n",
        "def createmodel():\n",
        "    # Initializing a Sequential model\n",
        "    model = Sequential()  # Sequential Neural Network\n",
        "    \n",
        "    # Adding an Embedding layer to the model\n",
        "    # Input dimension is set to max_fatures (2000 neurons)\n",
        "    # Output dimension is set to embed_dim (128 neurons)\n",
        "    # Input length is set to the number of columns in the feature matrix X\n",
        "    model.add(Embedding(max_fatures, embed_dim, input_length=X.shape[1]))  # Input dimension 2000 Neurons, output dimension 128 Neurons\n",
        "    \n",
        "    # Adding a Long Short-Term Memory (LSTM) layer to the model\n",
        "    # Number of neurons in the LSTM layer is set to lstm_out (196 neurons)\n",
        "    # Dropout is set to 20% for input and recurrent connections\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))  # Drop out 20%, 196 output Neurons, recurrent dropout 20%\n",
        "    \n",
        "    # Adding a Dense layer with 3 output neurons and softmax activation function\n",
        "    # The output represents the probabilities of each class (positive, neutral, negative)\n",
        "    model.add(Dense(3, activation='softmax'))  # 3 output neurons [positive, Neutral, Negative], softmax as activation\n",
        "    \n",
        "    # Compiling the model with categorical_crossentropy loss function, adam optimizer, and accuracy metric\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Compiling the model\n",
        "    \n",
        "    # Returning the compiled model\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cYEoVaQkkoWo"
      },
      "outputs": [],
      "source": [
        "# Initializing a LabelEncoder object to perform label encoding\n",
        "labelencoder = LabelEncoder()  # Applying label Encoding on the label matrix\n",
        "\n",
        "# Encoding the categorical labels in the 'sentiment' column of the DataFrame 'data' into integers\n",
        "# Fitting the label encoder to the labels and transforming them\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])  # fitting the model\n",
        "\n",
        "# Converting the integer-encoded labels into one-hot encoded vectors\n",
        "y = to_categorical(integer_encoded)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "# X_train and Y_train represent the features and labels for training, respectively\n",
        "# X_test and Y_test represent the features and labels for testing, respectively\n",
        "# The test_size parameter is set to 0.33, indicating a 33% test data split\n",
        "# The random_state parameter is set to 42 for reproducibility\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42)  # 67% training data, 33% test data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD3_8iCmkq51",
        "outputId": "4acb2b62-9a9d-4236-ea60-46513787a1ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "291/291 - 52s - loss: 0.8292 - accuracy: 0.6430 - 52s/epoch - 179ms/step\n",
            "144/144 - 4s - loss: 0.7674 - accuracy: 0.6619 - 4s/epoch - 28ms/step\n",
            "0.7674025893211365\n",
            "0.6618610620498657\n"
          ]
        }
      ],
      "source": [
        "# Setting the batch size for training to 32\n",
        "batch_size = 32  # Batch size 32\n",
        "\n",
        "# Creating the sequential neural network model\n",
        "model = createmodel()  # Function call to Sequential Neural Network\n",
        "\n",
        "# Training the model on the training data\n",
        "# Number of epochs is set to 1\n",
        "# Batch size is set to batch_size (32)\n",
        "# Verbose is set to 2 to display progress messages during training\n",
        "model.fit(X_train, Y_train, epochs=1, batch_size=batch_size, verbose=2)  # verbose the higher, the more messages\n",
        "\n",
        "# Evaluating the trained model on the test data\n",
        "# Verbose is set to 2 to display progress messages during evaluation\n",
        "# The evaluation score and accuracy are assigned to variables score and acc, respectively\n",
        "score, acc = model.evaluate(X_test, Y_test, verbose=2, batch_size=batch_size)  # evaluating the model\n",
        "\n",
        "# Printing the evaluation score and accuracy\n",
        "print(score)\n",
        "print(acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMZORpTvkuHJ",
        "outputId": "33ba4200-9e5b-47bc-ecdc-e56a05a45722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['loss', 'accuracy']\n"
          ]
        }
      ],
      "source": [
        "print(model.metrics_names) #metrics of the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT8K4AdslO_y"
      },
      "source": [
        "1. Save the model and use the saved model to predict on new text data (ex, “A lot of good things are happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump”)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3ewWC4Ikx_S",
        "outputId": "504219ac-39b9-4418-df26-ed5241131589"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('sentimentAnalysis.h5') #Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EjlUJxvzk46j"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model #Importing the package for importing the saved model\n",
        "model= load_model('sentimentAnalysis.h5') #loading the saved model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwFftwtAk8il",
        "outputId": "97e2b53e-7f4b-4249-ba52-6313125b70df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 2 1 ... 2 0 2]\n",
            "0         Neutral\n",
            "1        Positive\n",
            "2         Neutral\n",
            "3        Positive\n",
            "4        Positive\n",
            "           ...   \n",
            "13866    Negative\n",
            "13867    Positive\n",
            "13868    Positive\n",
            "13869    Negative\n",
            "13870    Positive\n",
            "Name: sentiment, Length: 13871, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(integer_encoded)\n",
        "print(data['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkyWh06sk_3V",
        "outputId": "231aa235-663c-4f13-b98d-fa0e8e93b3f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 - 0s - 294ms/epoch - 294ms/step\n",
            "[0.36646983 0.11932252 0.51420766]\n",
            "Positive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Predicting on the text data\n",
        "sentence = ['A lot of good things are happening. We are respected again throughout the world, and that is a great thing.@realDonaldTrump']\n",
        "sentence = tokenizer.texts_to_sequences(sentence) # Tokenizing the sentence\n",
        "sentence = pad_sequences(sentence, maxlen=28, dtype='int32', value=0) # Padding the sentence\n",
        "sentiment_probs = model.predict(sentence, batch_size=1, verbose=2)[0] # Predicting the sentence text\n",
        "sentiment = np.argmax(sentiment_probs)\n",
        "\n",
        "print(sentiment_probs)\n",
        "if sentiment == 0:\n",
        "    print(\"Neutral\")\n",
        "elif sentiment < 0:\n",
        "    print(\"Negative\")\n",
        "elif sentiment > 0:\n",
        "    print(\"Positive\")\n",
        "else:\n",
        "    print(\"Cannot be determined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puxfRL56lWBt"
      },
      "source": [
        "\n",
        "2. Apply GridSearchCV on the source code provided in the class\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfRTRDtLlZaR",
        "outputId": "275926f9-537f-46ed-c2db-cdcc7581dce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (24.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.4.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.12.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc0Sb9n2lcGL",
        "outputId": "4a3ea7fa-c949-472c-8e53-a826717138b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "744/744 - 116s - loss: 0.8256 - accuracy: 0.6504 - 116s/epoch - 155ms/step\n",
            "186/186 - 4s - 4s/epoch - 21ms/step\n",
            "744/744 - 100s - loss: 0.8205 - accuracy: 0.6451 - 100s/epoch - 134ms/step\n",
            "186/186 - 3s - 3s/epoch - 14ms/step\n",
            "744/744 - 96s - loss: 0.8212 - accuracy: 0.6464 - 96s/epoch - 129ms/step\n",
            "186/186 - 3s - 3s/epoch - 18ms/step\n",
            "744/744 - 128s - loss: 0.8300 - accuracy: 0.6430 - 128s/epoch - 172ms/step\n",
            "186/186 - 4s - 4s/epoch - 19ms/step\n",
            "744/744 - 135s - loss: 0.8187 - accuracy: 0.6512 - 135s/epoch - 182ms/step\n",
            "186/186 - 5s - 5s/epoch - 25ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 125s - loss: 0.8274 - accuracy: 0.6466 - 125s/epoch - 167ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 110s - loss: 0.6766 - accuracy: 0.7097 - 110s/epoch - 147ms/step\n",
            "186/186 - 4s - 4s/epoch - 23ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 103s - loss: 0.8204 - accuracy: 0.6481 - 103s/epoch - 139ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 97s - loss: 0.6734 - accuracy: 0.7143 - 97s/epoch - 130ms/step\n",
            "186/186 - 3s - 3s/epoch - 14ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 103s - loss: 0.8254 - accuracy: 0.6445 - 103s/epoch - 139ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 99s - loss: 0.6783 - accuracy: 0.7140 - 99s/epoch - 132ms/step\n",
            "186/186 - 3s - 3s/epoch - 15ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 98s - loss: 0.8287 - accuracy: 0.6448 - 98s/epoch - 131ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 98s - loss: 0.6765 - accuracy: 0.7104 - 98s/epoch - 132ms/step\n",
            "186/186 - 3s - 3s/epoch - 15ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 100s - loss: 0.8184 - accuracy: 0.6496 - 100s/epoch - 135ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 97s - loss: 0.6651 - accuracy: 0.7139 - 97s/epoch - 131ms/step\n",
            "186/186 - 3s - 3s/epoch - 14ms/step\n",
            "372/372 - 62s - loss: 0.8343 - accuracy: 0.6390 - 62s/epoch - 168ms/step\n",
            "93/93 - 3s - 3s/epoch - 32ms/step\n",
            "372/372 - 58s - loss: 0.8223 - accuracy: 0.6419 - 58s/epoch - 157ms/step\n",
            "93/93 - 2s - 2s/epoch - 24ms/step\n",
            "372/372 - 54s - loss: 0.8335 - accuracy: 0.6419 - 54s/epoch - 146ms/step\n",
            "93/93 - 2s - 2s/epoch - 19ms/step\n",
            "372/372 - 55s - loss: 0.8300 - accuracy: 0.6389 - 55s/epoch - 147ms/step\n",
            "93/93 - 3s - 3s/epoch - 31ms/step\n"
          ]
        }
      ],
      "source": [
        "from scikeras.wrappers import KerasClassifier #importing Keras classifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV #importing Grid search CV\n",
        "\n",
        "model = KerasClassifier(model=createmodel,verbose=2) #initiating model to test performance by applying multiple hyper parameters\n",
        "batch_size= [10, 20, 40] #hyper parameter batch_size\n",
        "epochs = [1, 2] #hyper parameter no. of epochs\n",
        "param_grid= {'batch_size':batch_size, 'epochs':epochs} #creating dictionary for batch size, no. of epochs\n",
        "grid  = GridSearchCV(estimator=model, param_grid=param_grid) #Applying dictionary with hyper parameters\n",
        "grid_result= grid.fit(X_train,Y_train) #Fitting the model\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) #best score, best hyper parameters\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
